{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import DependencyGraph\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import pymorphy2\n",
    "import math\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import time\n",
    "import codecs\n",
    "import os.path\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import artm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Trump_240.txt','r')\n",
    "t = open('trump_labelled.txt','w')\n",
    "i = 0\n",
    "texts = []\n",
    "for l in f.readlines():\n",
    "    t.write(str(i) + ' |text ')\n",
    "    text = l.split('\\t')[1]\n",
    "    if text in texts:\n",
    "        continue\n",
    "    texts.append(text)\n",
    "    t.write(text + '\\n')\n",
    "    t.write('|mark ' + l.split('\\t')[2]+'\\n')\n",
    "    i+=1\n",
    "f.close()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_sentences(conll_file):\n",
    "    processed_sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(conll_file, 'r', 'utf-8'):\n",
    "        if len(line) == 1:\n",
    "            processed_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            word = line.split(\"\\t\")\n",
    "            sentence.append(word)\n",
    "    return processed_sentences\n",
    "\n",
    "def get_raw_sentences(text_file):\n",
    "    sentences = []\n",
    "    for line in codecs.open(text_file, 'r', 'utf-8'):\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deps(processed_sentences):\n",
    "    deps = []\n",
    "    for sentence in processed_sentences:\n",
    "        s = u''\n",
    "        for line in sentence:\n",
    "            s += u\"\\t\".join(line) + u'\\n'\n",
    "        deps.append(s)\n",
    "    return deps\n",
    "\n",
    "def print_deps_tree(sent_dep):\n",
    "    graph = DependencyGraph(tree_str=sent_dep)\n",
    "    for triple in graph.triples():\n",
    "        for e in triple:\n",
    "            print(e[0]) if isinstance(e, tuple) else e,\n",
    "        print\n",
    "    print\n",
    "    tree = graph.tree()\n",
    "    print(tree.pretty_print())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding SPO triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforms conll lines into lists:\n",
    "def get_lists(sent_dep):\n",
    "    dependencies = []\n",
    "    pos = []\n",
    "    tp = []\n",
    "    words = []\n",
    "    for t in sent_dep.split('\\n'):\n",
    "        if len(t) > 1:\n",
    "            splt = t.split('\\t')\n",
    "            dependencies.append(int(splt[6]) - 1)\n",
    "            pos.append(splt[3])\n",
    "            tp.append(splt[7])\n",
    "            words.append(splt[1])\n",
    "            \n",
    "    for i in range(len(tp)):\n",
    "        # Find 'and' sequences\n",
    "        if tp[i] == 'conj' and pos[i] == 'VERB':\n",
    "            ids = [x for x in range(len(tp)) if dependencies[x] == dependencies[i] and tp[x] == 'nsubj'] \n",
    "            for j in ids:\n",
    "                words.append(words[j])\n",
    "                pos.append(pos[j])\n",
    "                tp.append(tp[j])\n",
    "                dependencies.append(i)\n",
    "        elif tp[i] == 'conj' and pos[i] != 'VERB':\n",
    "            dep = dependencies[i]\n",
    "            pos[i] = pos[dep]\n",
    "            dependencies[i] = dependencies[dep]\n",
    "            tp[i] = tp[dep]\n",
    "            \n",
    "        # Find complex verbs\n",
    "        if tp[i] in ['xcomp','dep']:\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[dep] + ' ' + words[i]\n",
    "            ids = [x for x in range(len(tp)) if dependencies[x] == i]\n",
    "            for j in ids:\n",
    "                dependencies[j] = dep\n",
    "            pos[dep] = u'VERB'\n",
    "            pos[i] = 'ADD_VERB'\n",
    "            tp[i] = 'ADD_VERB'\n",
    "            \n",
    "        # Adjective triplets\n",
    "        if tp[i] == 'ADJ' and pos[dependencies[i]] == 'VERB':\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[dep]+' '+words[i]\n",
    "        \n",
    "        # Determine negative verbs\n",
    "        if tp[i] == u'neg':\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[i]+' '+words[dep]\n",
    "        \n",
    "        # Substitude words with their names if present\n",
    "        if tp[i] == u'name':\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[i]\n",
    "\n",
    "#         if u'котор' in words[i]:\n",
    "#             dep = int(dependencies[i]) - 1\n",
    "#             words[i] = words[dep]\n",
    "#             print words[i]\n",
    "    return words, pos, dependencies, tp\n",
    "            \n",
    "                \n",
    "# Find triplets in conll processed form        \n",
    "def get_triplets(processed_sentence):\n",
    "    triplets = []\n",
    "    sent_dep = u''\n",
    "    for line in processed_sentence:\n",
    "        sent_dep += u\"\\t\".join(line) + u'\\n'\n",
    "    words, pos, dependencies, tp = get_lists(sent_dep)\n",
    "    \n",
    "    ids = range(len(words))\n",
    "    \n",
    "    # regular triplets\n",
    "    verbs = [x for x in ids if pos[x] == u'VERB' and tp[x] != 'amod']\n",
    "    for i in verbs:\n",
    "        verb_subjects = [words[x] for x in ids if tp[x] in ['nsubj','nsubjpass'] and dependencies[x] == i]\n",
    "        if len(verb_subjects) == 0:\n",
    "            verb_subjects.append(u'imp')\n",
    "        verb_objects = [words[x] for x in ids if tp[x] == 'dobj' and dependencies[x] == i]\n",
    "        if len(verb_objects) == 0:\n",
    "            verb_objects.append(u'imp')\n",
    "        for subj, obj in itertools.product(verb_subjects, verb_objects):\n",
    "            triplets.append([subj, words[i], obj])\n",
    "       \n",
    "    # participle triplets\n",
    "    participles = [x for x in ids if pos[x] == u'VERB' and tp[x] == 'amod']\n",
    "    for i in participles:\n",
    "        participle_subjects = [words[x] for x in ids if dependencies[i] == x]\n",
    "        if len(participle_subjects) == 0:\n",
    "            participle_subjects.append(u'imp')\n",
    "        participle_objects = [words[x] for x in ids if tp[x] == 'dobj' and dependencies[x] == i]\n",
    "        if len(participle_objects) == 0:\n",
    "            participle_objects.append(u'imp')\n",
    "        for subj, obj in itertools.product(participle_subjects, participle_objects):\n",
    "            triplets.append([subj, words[i], obj])\n",
    "            \n",
    "    # implicit noun-noun triplets\n",
    "    appos = [x for x in ids if tp[x] == u'appos']\n",
    "    for i in appos:\n",
    "        obj = words[dependencies[i]]\n",
    "        triplets.append([words[i], u'есть', obj])\n",
    "\n",
    "                \n",
    "    #adjectives triplets\n",
    "    adjectives = [x for x in ids if pos[x] == 'ADJ' and tp[x] == 'amod']\n",
    "    for adj in adjectives:\n",
    "        triplets.append([words[dependencies[adj]], u'есть', words[adj]])\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def print_triplets(triplets_list):\n",
    "    for i, triplet in enumerate(triplets_list):\n",
    "        print(str(i + 1), '(' + triplet[0],', ', triplet[1],', ', triplet[2] + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess raw text for syntaxnet input\n",
    "def syntaxnet_preprocess(filename):\n",
    "    f = codecs.open(filename + '.txt', 'r')\n",
    "    t = open(filename + '_prepared.txt','w')\n",
    "    for line in f.readlines():\n",
    "        line = re.sub(r'([.,!?()])', r' \\1 ', line)\n",
    "        line = re.sub('  ',' ',line)\n",
    "        line = re.sub('«', '', line)\n",
    "        line = re.sub('»', '', line)\n",
    "        line = re.sub('\"', '', line)\n",
    "        line = re.sub('-', '', line)\n",
    "        \n",
    "        line = line.replace(r'. ', '.\\n')\n",
    "        t.write(line)\n",
    "        \n",
    "\n",
    "def run_syntaxnet(textfile, conllfile):\n",
    "    command = \"cat \" + textfile + \" | docker run --rm -i inemo/syntaxnet_rus > \" + conllfile\n",
    "    os.system(command)\n",
    "    \n",
    "# Get triplets from text doc or conll doc    \n",
    "def get_doc_triplets(filename, conll = False):\n",
    "    if conll == False: \n",
    "        syntaxnet_preprocess(filename)\n",
    "        run_syntaxnet(filename + '_prepared.txt', filename + '.conll')\n",
    "    processed_sentences = get_processed_sentences(filename + '.conll')\n",
    "    text_triplets = []\n",
    "    for sent in processed_sentences:\n",
    "        text_triplets.extend(get_triplets(sent))\n",
    "    return text_triplets\n",
    "\n",
    "# Extract all subjects from triplet list\n",
    "def subjects_from_triplets(triplet_list):\n",
    "    stop_words = get_stop_words('russian')\n",
    "    return [x[0] for x in triplet_list if x[0] != u'imp' and x[0] not in stop_words]\n",
    "\n",
    "# Extract all objects from triplet list\n",
    "def objects_from_triplets(triplet_list):\n",
    "    stop_words = get_stop_words('russian')\n",
    "    return [x[2] for x in triplet_list if x[2] != u'imp' and x[2] not in stop_words]\n",
    "\n",
    "\n",
    "def get_subjects_from_triplet_lists(triplet_lists):\n",
    "    subject_lists = []\n",
    "    for triplets in triplet_lists:\n",
    "        subject_lists.append(subjects_from_triplets(triplets))\n",
    "    return subject_lists\n",
    "\n",
    "# Lemmatize each triplet in triplet list\n",
    "def lemmatize_triplet_list(triplet_list):\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    stop_words = get_stop_words('russian')\n",
    "    for i, triplet in enumerate(triplet_list):\n",
    "        triplet_list[i] = [lemmatizer.parse(token)[0].normal_form.strip()\n",
    "                           for token in triplet]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic modeling with labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_vw(exists = True):\n",
    "    marks = []\n",
    "    \n",
    "    if exists == True:\n",
    "        f = open('news_vw','r')\n",
    "        for line in f.readlines():\n",
    "            marks.append(int(line.split('|mark ')[-1]))\n",
    "        f.close()\n",
    "        return marks\n",
    "    \n",
    "    \n",
    "    output_file = open('news_vw', 'w')\n",
    "\n",
    "    f = codecs.open('trump_labelled.txt','r')\n",
    "    i = 0\n",
    "    j = -1\n",
    "    for line in tqdm(f.readlines()):\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        j+=1\n",
    "        \n",
    "        if j%2 == 0:\n",
    "            text = line.split('|text')[1]\n",
    "            num = line.split('|text')[0]\n",
    "            textfile = codecs.open('trump_text.txt','w')\n",
    "            textfile.write(text)\n",
    "            textfile.close()\n",
    "\n",
    "            triplets = get_doc_triplets('trump_text', conll = False)\n",
    "            lemmatize_triplet_list(triplets)\n",
    "            subjects = subjects_from_triplets(triplets)\n",
    "            objects = objects_from_triplets(triplets)\n",
    "            \n",
    "        else:\n",
    "            mark = line.split('|mark')[1]\n",
    "            if mark != '  9\\n':\n",
    "                marks.append(int(mark))\n",
    "                output_file.write(str(i + 1) + \" |subjects \")\n",
    "                for subject in set(subjects):\n",
    "                    if subject == u'—':\n",
    "                        continue\n",
    "                    subject = re.sub(':', '', subject)\n",
    "                    output_file.write(' ' + subject.lower())\n",
    "                    output_file.write(':' + str(subjects.count(subject)))\n",
    "                    \n",
    "                output_file.write(\" |objects \")\n",
    "                for obj in set(objects):\n",
    "                    if obj == u'—':\n",
    "                        continue\n",
    "                    obj = re.sub(':', '', obj)\n",
    "                    output_file.write(' ' + obj.lower())\n",
    "                    output_file.write(':' + str(objects.count(obj)))\n",
    "                    \n",
    "\n",
    "                output_file.write(\" |mark \" + mark)\n",
    "            i+=1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    output_file.close()\n",
    "    return marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks = prepare_vw(exists = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './news_vw'\n",
    "batches_path = './batches/'\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=data_path, collection_name='',\n",
    "                                            data_format='vowpal_wabbit', batch_size = 100, \n",
    "                                            target_folder=batches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=batches_path, \n",
    "                                         data_format='batches',\n",
    "                                       gather_dictionary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_model(num_of_topics, num_back, tau, tf):\n",
    "    class_ids = {\n",
    "         'subjects': 0.7,\n",
    "         'objects':0.3\n",
    "    }\n",
    "\n",
    "    names_of_topics = [str(x) for x in range(num_of_topics)]\n",
    "\n",
    "    dictionary_path=batches_path + '/news_dictionary.dict'\n",
    "\n",
    "    my_dictionary = artm.Dictionary()\n",
    "\n",
    "    if os.path.exists(dictionary_path):\n",
    "        os.remove(dictionary_path)\n",
    "        \n",
    "    my_dictionary.gather(data_path=batches_path)\n",
    "    my_dictionary.save(dictionary_path=batches_path + '/news_dictionary')\n",
    "    my_dictionary.load(dictionary_path=batches_path + '/news_dictionary.dict')\n",
    "\n",
    "    my_dictionary.filter(min_tf=tf)\n",
    "\n",
    "    scores_artm = [artm.PerplexityScore(name='PerplexityScore', \n",
    "                                        dictionary=my_dictionary,class_ids = class_ids\n",
    "                                       ),\n",
    "                   artm.TopTokensScore(name='TopTokensScore', \n",
    "                                       topic_names=names_of_topics, \n",
    "                                       num_tokens=1000, \n",
    "                                       dictionary=my_dictionary,\n",
    "                                       class_id='text'\n",
    "                                      )]\n",
    "\n",
    "    model = artm.ARTM(num_topics=num_of_topics,\n",
    "                      #reuse_theta=True,\n",
    "                      num_document_passes=1,\n",
    "                      topic_names=names_of_topics,\n",
    "                      class_ids=class_ids, \n",
    "                      scores=scores_artm,\n",
    "                      cache_theta = True,\n",
    "                      #regularizers=regularizers_artm,\n",
    "                      dictionary=my_dictionary)\n",
    "\n",
    "\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhiRegularizer',\n",
    "                                                            class_ids=class_ids,\n",
    "                                                            topic_names=model.topic_names[:-num_back],tau = -tau))\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SmoothPhiRegularizer',\n",
    "                                                            class_ids=class_ids,\n",
    "                                                            topic_names=model.topic_names[-num_back:],tau = tau))\n",
    "\n",
    "\n",
    "#     model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='DecorrelatorRegularizer',\n",
    "#                                                           class_ids=['@default_class'],\n",
    "#                                                           topic_names=model.topic_names[:-num_back], tau=tau))\n",
    "    model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SparseThetaRegularizer',\n",
    "                                                            topic_names=model.topic_names[:-num_back], tau = tau))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_clusters(y_true, y_pred):\n",
    "    m = {}\n",
    "    clusters = set(y_pred)\n",
    "    for c1 in clusters:\n",
    "        cnt1 = 0\n",
    "        for c2 in set(y_true): \n",
    "            \n",
    "            cnt = 0\n",
    "            for (x,y) in zip(y_pred,y_true):\n",
    "                if (x==c1) & (y==c2):\n",
    "                    cnt+=1\n",
    "            if cnt>cnt1:\n",
    "                cnt1 = cnt\n",
    "                res = c2\n",
    "        m[c1] = res\n",
    "    return m\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "def precision_recall(y_true,y_pred):\n",
    "    m = map_clusters(y_true,y_pred)\n",
    "    if len(set(m.values()))<len(set(y_true)):\n",
    "        return 0,0\n",
    "    y_pred = np.array([m[x] for x in y_pred])\n",
    "    precision = metrics.precision_score(y_true,y_pred,average='weighted')\n",
    "    recall = metrics.recall_score(y_true,y_pred,average='weighted')\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "def without_zero(y_true,X):\n",
    "    a, b = [],[]\n",
    "    for (x,y) in zip(y_true,X):\n",
    "        if x ==0:\n",
    "            continue\n",
    "        a.append(x)\n",
    "        b.append(y)\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=2).fit(b)\n",
    "    return np.array(a),np.array(kmeans.labels_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:42<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "{'min_tf': 3, 'num_back': 2, 'num_topics': 5, 'tau': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param['num_topics'] = [x for x in range(3,6)]\n",
    "param['num_back'] = [x for x in range(1,3)]\n",
    "param['min_tf'] = [x for x in range(1,5)]\n",
    "param['tau'] = [x for x in np.arange(1.,4.,2.)]\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "pg = ParameterGrid(param)\n",
    "best = [0,0]\n",
    "\n",
    "for p in tqdm(list(pg)):\n",
    "    \n",
    "    model = topic_model(p['num_topics'], p['num_back'],p['tau'],p['min_tf'])\n",
    "    model.fit_offline(batch_vectorizer, num_collection_passes=30)\n",
    "    \n",
    "    theta = model.get_theta()\n",
    "    X = theta.as_matrix()[:-p['num_back']].T\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=3).fit(X)\n",
    "    y_pred = kmeans.labels_\n",
    "    y_true = np.array(marks)\n",
    "    \n",
    "    res = precision_recall(y_true,y_pred)\n",
    "    \n",
    "    \n",
    "    if res[0] > best[0]:\n",
    "        best = res\n",
    "        best_p = p\n",
    "    \n",
    "print(best)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:26<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.61633302440531357, 0.60499999999999998)\n",
      "{'min_tf': 3, 'num_back': 2, 'num_topics': 5, 'tau': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best = [0,0]\n",
    "\n",
    "for p in tqdm(list(pg)):\n",
    "    \n",
    "    model = topic_model(p['num_topics'], p['num_back'],p['tau'],p['min_tf'])\n",
    "    model.fit_offline(batch_vectorizer, num_collection_passes=30)\n",
    "    \n",
    "    theta = model.get_theta()\n",
    "    X = theta.as_matrix()[:-p['num_back']].T\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=3).fit(X)\n",
    "    y_pred = kmeans.labels_\n",
    "    y_true = np.array(marks)\n",
    "    \n",
    "    \n",
    "    a,b = without_zero(y_true,X)\n",
    "    \n",
    "    res = precision_recall(a,b)\n",
    "    \n",
    "    if res[0] > best[0]:\n",
    "        best = res\n",
    "        best_p = p\n",
    "    \n",
    "print(best)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "соглашение         0.380936\n",
       "парижский          0.276670\n",
       "трамп              0.118607\n",
       "соглашение         0.111783\n",
       "президент          0.080608\n",
       "президент          0.068580\n",
       "сша                0.060938\n",
       "климатический      0.058679\n",
       "решение            0.049376\n",
       "американский       0.048942\n",
       "глобальный         0.047005\n",
       "штат               0.037427\n",
       "страна             0.032507\n",
       "лидер              0.031869\n",
       "переговоры         0.030753\n",
       "дональд            0.027600\n",
       "дональд            0.027168\n",
       "парниковый         0.026812\n",
       "потепление         0.023658\n",
       "сша                0.023082\n",
       "мировой            0.022276\n",
       "рабочий            0.021485\n",
       "место              0.021437\n",
       "маск               0.020614\n",
       "совет              0.019344\n",
       "дом                0.019257\n",
       "совет              0.017738\n",
       "решение            0.017454\n",
       "газ                0.016791\n",
       "условие            0.016294\n",
       "                     ...   \n",
       "победа             0.000000\n",
       "создание           0.000000\n",
       "величие            0.000000\n",
       "видный             0.000000\n",
       "обязать            0.000000\n",
       "работа             0.000000\n",
       "ущерб              0.000000\n",
       "намерение          0.000000\n",
       "необратимый        0.000000\n",
       "обращение          0.000000\n",
       "цельсий            0.000000\n",
       "вернуть            0.000000\n",
       "welle              0.000000\n",
       "сообщество         0.000000\n",
       "въезд              0.000000\n",
       "пустой             0.000000\n",
       "суббота            0.000000\n",
       "сохранение         0.000000\n",
       "обязанность        0.000000\n",
       "эра                0.000000\n",
       "план               0.000000\n",
       "участник           0.000000\n",
       "противоположный    0.000000\n",
       "ребёнок            0.000000\n",
       "актёр              0.000000\n",
       "мир                0.000000\n",
       "ничто              0.000000\n",
       "планет             0.000000\n",
       "великий            0.000000\n",
       "защита             0.000000\n",
       "Name: 4, Length: 1110, dtype: float32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_phi()['4'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 0, 2, 2, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 2, 0, 1, 1, 1, 0, 0,\n",
       "        1, 2, 1, 0, 0, 1, 0, 2, 1, 1, 0, 2, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 1,\n",
       "        0, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1,\n",
       "        1, 1, 1, 2, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 1, 2,\n",
       "        0, 2, 1, 0, 0, 0, 2, 2, 1, 1, 0, 2, 0, 0, 1, 1, 0, 2, 2, 1, 0, 0, 0,\n",
       "        2, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 1,\n",
       "        2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 1,\n",
       "        1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 1,\n",
       "        0, 1, 1, 1, 1, 2, 0, 2, 1, 1], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 2, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 0,\n",
       "        1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 2, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 0, 0, 0, 2, 2, 0, 1, 0, 2, 1, 0,\n",
       "        2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 1, 2, 2,\n",
       "        1, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2,\n",
       "        2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2,\n",
       "        2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2,\n",
       "        1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1,\n",
       "        1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1,\n",
       "        2, 1, 2, 1, 1, 1, 2, 2, 1, 1]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred,y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
