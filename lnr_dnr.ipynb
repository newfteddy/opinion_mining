{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import DependencyGraph\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import pymorphy2\n",
    "import math\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import time\n",
    "import codecs\n",
    "import os.path\n",
    "from sklearn import metrics\n",
    "\n",
    "import artm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_processed_sentences(conll_file):\n",
    "    processed_sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(conll_file, 'r', 'utf-8'):\n",
    "        if len(line) == 1:\n",
    "            processed_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            word = line.split(\"\\t\")\n",
    "            sentence.append(word)\n",
    "    return processed_sentences\n",
    "\n",
    "def get_raw_sentences(text_file):\n",
    "    sentences = []\n",
    "    for line in codecs.open(text_file, 'r', 'utf-8'):\n",
    "        sentences.append(line)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deps(processed_sentences):\n",
    "    deps = []\n",
    "    for sentence in processed_sentences:\n",
    "        s = u''\n",
    "        for line in sentence:\n",
    "            s += u\"\\t\".join(line) + u'\\n'\n",
    "        deps.append(s)\n",
    "    return deps\n",
    "\n",
    "def print_deps_tree(sent_dep):\n",
    "    graph = DependencyGraph(tree_str=sent_dep)\n",
    "    for triple in graph.triples():\n",
    "        for e in triple:\n",
    "            print(e[0]) if isinstance(e, tuple) else e,\n",
    "        print\n",
    "    print\n",
    "    tree = graph.tree()\n",
    "    print(tree.pretty_print())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finding SPO triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforms conll lines into lists:\n",
    "def get_lists(sent_dep):\n",
    "    dependencies = []\n",
    "    pos = []\n",
    "    tp = []\n",
    "    words = []\n",
    "    for t in sent_dep.split('\\n'):\n",
    "        if len(t) > 1:\n",
    "            splt = t.split('\\t')\n",
    "            dependencies.append(int(splt[6]) - 1)\n",
    "            pos.append(splt[3])\n",
    "            tp.append(splt[7])\n",
    "            words.append(splt[1])\n",
    "            \n",
    "    for i in range(len(tp)):\n",
    "        # Find 'and' sequences\n",
    "        if tp[i] == 'conj' and pos[i] == 'VERB':\n",
    "            ids = [x for x in range(len(tp)) if dependencies[x] == dependencies[i] and tp[x] == 'nsubj'] \n",
    "            for j in ids:\n",
    "                words.append(words[j])\n",
    "                pos.append(pos[j])\n",
    "                tp.append(tp[j])\n",
    "                dependencies.append(i)\n",
    "        elif tp[i] == 'conj' and pos[i] != 'VERB':\n",
    "            dep = dependencies[i]\n",
    "            pos[i] = pos[dep]\n",
    "            dependencies[i] = dependencies[dep]\n",
    "            tp[i] = tp[dep]\n",
    "            \n",
    "        # Find complex verbs\n",
    "        if tp[i] in ['xcomp','dep']:\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[dep] + ' ' + words[i]\n",
    "            ids = [x for x in range(len(tp)) if dependencies[x] == i]\n",
    "            for j in ids:\n",
    "                dependencies[j] = dep\n",
    "            pos[dep] = u'VERB'\n",
    "            pos[i] = 'ADD_VERB'\n",
    "            tp[i] = 'ADD_VERB'\n",
    "            \n",
    "        # Adjective triplets\n",
    "        if tp[i] == 'ADJ' and pos[dependencies[i]] == 'VERB':\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[dep]+' '+words[i]\n",
    "        \n",
    "        # Determine negative verbs\n",
    "        if tp[i] == u'neg':\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[i]+' '+words[dep]\n",
    "        \n",
    "        # Substitude words with their names if present\n",
    "        if tp[i] == u'name':\n",
    "            dep = dependencies[i]\n",
    "            words[dep] = words[i]\n",
    "\n",
    "#         if u'котор' in words[i]:\n",
    "#             dep = int(dependencies[i]) - 1\n",
    "#             words[i] = words[dep]\n",
    "#             print words[i]\n",
    "    return words, pos, dependencies, tp\n",
    "            \n",
    "                \n",
    "# Find triplets in conll processed form        \n",
    "def get_triplets(processed_sentence):\n",
    "    triplets = []\n",
    "    sent_dep = u''\n",
    "    for line in processed_sentence:\n",
    "        sent_dep += u\"\\t\".join(line) + u'\\n'\n",
    "    words, pos, dependencies, tp = get_lists(sent_dep)\n",
    "    \n",
    "    ids = range(len(words))\n",
    "    \n",
    "    # regular triplets\n",
    "    verbs = [x for x in ids if pos[x] == u'VERB' and tp[x] != 'amod']\n",
    "    for i in verbs:\n",
    "        verb_subjects = [words[x] for x in ids if tp[x] in ['nsubj','nsubjpass'] and dependencies[x] == i]\n",
    "        if len(verb_subjects) == 0:\n",
    "            verb_subjects.append(u'imp')\n",
    "        verb_objects = [words[x] for x in ids if tp[x] == 'dobj' and dependencies[x] == i]\n",
    "        if len(verb_objects) == 0:\n",
    "            verb_objects.append(u'imp')\n",
    "        for subj, obj in itertools.product(verb_subjects, verb_objects):\n",
    "            triplets.append([subj, words[i], obj])\n",
    "       \n",
    "    # participle triplets\n",
    "    participles = [x for x in ids if pos[x] == u'VERB' and tp[x] == 'amod']\n",
    "    for i in participles:\n",
    "        participle_subjects = [words[x] for x in ids if dependencies[i] == x]\n",
    "        if len(participle_subjects) == 0:\n",
    "            participle_subjects.append(u'imp')\n",
    "        participle_objects = [words[x] for x in ids if tp[x] == 'dobj' and dependencies[x] == i]\n",
    "        if len(participle_objects) == 0:\n",
    "            participle_objects.append(u'imp')\n",
    "        for subj, obj in itertools.product(participle_subjects, participle_objects):\n",
    "            triplets.append([subj, words[i], obj])\n",
    "            \n",
    "    # implicit noun-noun triplets\n",
    "    appos = [x for x in ids if tp[x] == u'appos']\n",
    "    for i in appos:\n",
    "        obj = words[dependencies[i]]\n",
    "        triplets.append([words[i], u'есть', obj])\n",
    "\n",
    "                \n",
    "    #adjectives triplets\n",
    "    adjectives = [x for x in ids if pos[x] == 'ADJ' and tp[x] == 'amod']\n",
    "    for adj in adjectives:\n",
    "        triplets.append([words[dependencies[adj]], u'есть', words[adj]])\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def print_triplets(triplets_list):\n",
    "    for i, triplet in enumerate(triplets_list):\n",
    "        print(str(i + 1), '(' + triplet[0],', ', triplet[1],', ', triplet[2] + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess raw text for syntaxnet input\n",
    "def syntaxnet_preprocess(filename):\n",
    "    f = codecs.open(filename + '.txt', 'r')\n",
    "    t = open(filename + '_prepared.txt','w')\n",
    "    for line in f.readlines():\n",
    "        line = re.sub(r'([.,!?()])', r' \\1 ', line)\n",
    "        line = re.sub('  ',' ',line)\n",
    "        line = re.sub('«', '', line)\n",
    "        line = re.sub('»', '', line)\n",
    "        line = re.sub('\"', '', line)\n",
    "        line = re.sub('-', '', line)\n",
    "        \n",
    "        line = line.replace(r'. ', '.\\n')\n",
    "        t.write(line)\n",
    "        \n",
    "\n",
    "def run_syntaxnet(textfile, conllfile):\n",
    "    command = \"cat \" + textfile + \" | docker run --rm -i inemo/syntaxnet_rus > \" + conllfile\n",
    "    os.system(command)\n",
    "    \n",
    "# Get triplets from text doc or conll doc    \n",
    "def get_doc_triplets(filename, conll = False):\n",
    "    if conll == False: \n",
    "        syntaxnet_preprocess(filename)\n",
    "        run_syntaxnet(filename + '_prepared.txt', filename + '.conll')\n",
    "    processed_sentences = get_processed_sentences(filename + '.conll')\n",
    "    text_triplets = []\n",
    "    for sent in processed_sentences:\n",
    "        text_triplets.extend(get_triplets(sent))\n",
    "    return text_triplets\n",
    "\n",
    "# Extract all subjects from triplet list\n",
    "def subjects_from_triplets(triplet_list):\n",
    "    stop_words = get_stop_words('russian')\n",
    "    return [x[0] for x in triplet_list if x[0] != u'imp' and x[0] not in stop_words]\n",
    "\n",
    "# Extract all objects from triplet list\n",
    "def objects_from_triplets(triplet_list):\n",
    "    stop_words = get_stop_words('russian')\n",
    "    return [x[2] for x in triplet_list if x[2] != u'imp' and x[2] not in stop_words]\n",
    "\n",
    "\n",
    "def get_subjects_from_triplet_lists(triplet_lists):\n",
    "    subject_lists = []\n",
    "    for triplets in triplet_lists:\n",
    "        subject_lists.append(subjects_from_triplets(triplets))\n",
    "    return subject_lists\n",
    "\n",
    "# Lemmatize each triplet in triplet list\n",
    "def lemmatize_triplet_list(triplet_list):\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    stop_words = get_stop_words('russian')\n",
    "    for i, triplet in enumerate(triplet_list):\n",
    "        triplet_list[i] = [lemmatizer.parse(token)[0].normal_form.strip()\n",
    "                           for token in triplet]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic modeling with labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_vw(exists = True):\n",
    "    marks = []\n",
    "    \n",
    "    if exists == True:\n",
    "        f = open('news_vw','r')\n",
    "        for line in f.readlines():\n",
    "            marks.append(int(line.split('|mark ')[-1]))\n",
    "        f.close()\n",
    "        return marks\n",
    "    \n",
    "    \n",
    "    output_file = open('news_vw', 'w')\n",
    "\n",
    "    f = codecs.open('lnr_dnr_labelled.txt','r')\n",
    "    i = 0\n",
    "    j = -1\n",
    "    for line in f.readlines():\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        j+=1\n",
    "        \n",
    "        if j%2 == 0:\n",
    "            text = line.split('|text')[1]\n",
    "            num = line.split('|text')[0]\n",
    "            textfile = codecs.open('lnr_dnr_text.txt','w')\n",
    "            textfile.write(text)\n",
    "            textfile.close()\n",
    "\n",
    "            triplets = get_doc_triplets('lnr_dnr_text', conll = False)\n",
    "            lemmatize_triplet_list(triplets)\n",
    "            subjects = subjects_from_triplets(triplets)\n",
    "            objects = objects_from_triplets(triplets)\n",
    "            \n",
    "        else:\n",
    "            mark = line.split('|mark')[1]\n",
    "            if mark != '  9\\n':\n",
    "                marks.append(int(mark))\n",
    "                output_file.write(str(i + 1) + \" |subjects \")\n",
    "                for subject in set(subjects):\n",
    "                    if subject == u'—':\n",
    "                        continue\n",
    "                    subject = re.sub(':', '', subject)\n",
    "                    output_file.write(' ' + subject.lower().encode('utf8'))\n",
    "                    output_file.write(':' + str(subjects.count(subject)))\n",
    "                    \n",
    "                output_file.write(\" |objects \")\n",
    "                for obj in set(objects):\n",
    "                    if obj == u'—':\n",
    "                        continue\n",
    "                    obj = re.sub(':', '', obj)\n",
    "                    output_file.write(' ' + obj.lower().encode('utf8'))\n",
    "                    output_file.write(':' + str(objects.count(obj)))\n",
    "                    \n",
    "\n",
    "                output_file.write(\" |mark \" + mark)\n",
    "            i+=1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    output_file.close()\n",
    "    return marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('news_vw','r')\n",
    "t = open('lnr_dnr_vw','w')\n",
    "i=0\n",
    "for line in f.readlines():\n",
    "    t.write(str(i)+' |subjects'+line.split('|subjects')[1])\n",
    "    i+=1\n",
    "t.write('\\n')\n",
    "f.close()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marks = prepare_vw(exists = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './lnr_dnr_vw'\n",
    "batches_path = './batches/'\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=data_path, collection_name='',\n",
    "                                            data_format='vowpal_wabbit', batch_size = 100, \n",
    "                                            target_folder=batches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=batches_path, \n",
    "                                         data_format='batches',\n",
    "                                       gather_dictionary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_model(num_of_topics, num_back, tau, tf):\n",
    "    class_ids = {\n",
    "         'subjects': 0.7,\n",
    "         'objects':0.3\n",
    "    }\n",
    "\n",
    "    names_of_topics = [str(x) for x in range(num_of_topics)]\n",
    "\n",
    "    dictionary_path=batches_path + '/news_dictionary.dict'\n",
    "\n",
    "    my_dictionary = artm.Dictionary()\n",
    "\n",
    "    if os.path.exists(dictionary_path):\n",
    "        os.remove(dictionary_path)\n",
    "        \n",
    "    my_dictionary.gather(data_path=batches_path)\n",
    "    my_dictionary.save(dictionary_path=batches_path + '/news_dictionary')\n",
    "    my_dictionary.load(dictionary_path=batches_path + '/news_dictionary.dict')\n",
    "\n",
    "    my_dictionary.filter(min_tf=tf)\n",
    "\n",
    "    scores_artm = [artm.PerplexityScore(name='PerplexityScore', \n",
    "                                        dictionary=my_dictionary,class_ids=class_ids\n",
    "                                       ),\n",
    "                   artm.TopTokensScore(name='TopTokensScore', \n",
    "                                       topic_names=names_of_topics, \n",
    "                                       num_tokens=1000, \n",
    "                                       dictionary=my_dictionary,\n",
    "                                       class_id='text'\n",
    "                                      )]\n",
    "\n",
    "    model = artm.ARTM(num_topics=num_of_topics,\n",
    "                      #reuse_theta=True,\n",
    "                      cache_theta=True,\n",
    "                      num_document_passes=1,\n",
    "                      topic_names=names_of_topics,\n",
    "                      class_ids=class_ids, \n",
    "                      scores=scores_artm,\n",
    "                      #regularizers=regularizers_artm,\n",
    "                      dictionary=my_dictionary)\n",
    "\n",
    "\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhiRegularizer',\n",
    "                                                            class_ids=['@default_class'],\n",
    "                                                            topic_names=model.topic_names[:-num_back],tau = -tau))\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SmoothPhiRegularizer',\n",
    "                                                            class_ids=['@default_class'],\n",
    "                                                            topic_names=model.topic_names[-num_back:],tau = tau))\n",
    "\n",
    "\n",
    "    model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='DecorrelatorRegularizer',\n",
    "                                                          class_ids=['@default_class'],\n",
    "                                                          topic_names=model.topic_names[:-num_back], tau=tau*10))\n",
    "    model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SparseThetaRegularizer',\n",
    "                                                            topic_names=model.topic_names[-num_back], tau = tau))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_clusters(y_true, y_pred):\n",
    "    m = {}\n",
    "    clusters = set(y_true)\n",
    "    for c1 in clusters:\n",
    "        cnt1 = 0\n",
    "        for c2 in set(y_pred): \n",
    "            \n",
    "            cnt = 0\n",
    "            for (x,y) in zip(y_true,y_pred):\n",
    "                if (x==c1) & (y==c2):\n",
    "                    cnt+=1\n",
    "            if cnt>cnt1:\n",
    "                cnt1 = cnt\n",
    "                res = c2\n",
    "        m[c1] = res\n",
    "    return m\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "def precision_recall(y_true,y_pred):\n",
    "    m = map_clusters(y_true,y_pred)\n",
    "    if len(set(m.values()))<len(set(y_true)):\n",
    "        return 0,0\n",
    "    y_true = np.array([m[x] for x in y_true])\n",
    "    precision = metrics.precision_score(y_true,y_pred,average='weighted')\n",
    "    recall = metrics.recall_score(y_true,y_pred,average='weighted')\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "def without_zero(y_true,X):\n",
    "    a, b = [],[]\n",
    "    for (x,y) in zip(y_true,X):\n",
    "        if x ==0:\n",
    "            continue\n",
    "        a.append(x)\n",
    "        b.append(y)\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=2).fit(b)\n",
    "    return np.array(a),np.array(kmeans.labels_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[0, 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-018a25f237c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_p' is not defined"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param['num_topics'] = [x for x in range(3,6)]\n",
    "param['num_back'] = [x for x in range(1,3)]\n",
    "param['min_tf'] = [x for x in range(1,4)]\n",
    "param['tau'] = [x for x in np.arange(1.,10.,2.)]\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "pg = ParameterGrid(param)\n",
    "print(len(list(pg)))\n",
    "\n",
    "best = [0,0]\n",
    "\n",
    "for p in list(pg):\n",
    "    \n",
    "    model = topic_model(p['num_topics'], p['num_back'],p['tau'],p['min_tf'])\n",
    "    model.fit_offline(batch_vectorizer, num_collection_passes=30)\n",
    "    \n",
    "    theta = model.get_theta()\n",
    "    X = theta.as_matrix()[:-p['num_back']].T\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=3).fit(X)\n",
    "    y_pred = kmeans.labels_\n",
    "    y_true = np.array(marks)\n",
    "    \n",
    "    res = precision_recall(y_true,y_pred)\n",
    "    \n",
    "    \n",
    "    if res[0] > best[0]:\n",
    "        best = res\n",
    "        best_p = p\n",
    "    \n",
    "print(best)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = topic_model(5,1,1,3)\n",
    "model.fit_offline(batch_vectorizer, num_collection_passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "управление          0.110964\n",
      "внешний             0.100842\n",
      "завод               0.094606\n",
      "украинский          0.075484\n",
      "власть              0.042637\n",
      "работа              0.041684\n",
      "металлургический    0.041324\n",
      "предприятие         0.040541\n",
      "глава               0.039139\n",
      "республика          0.038746\n",
      "Name: 0, dtype: float32\n",
      "блокада              0.097436\n",
      "республика           0.090013\n",
      "область              0.079313\n",
      "самопровозгласить    0.066125\n",
      "луганский            0.059843\n",
      "управление           0.051056\n",
      "внешний              0.044955\n",
      "песок                0.044756\n",
      "донецкий             0.044178\n",
      "украинский           0.031704\n",
      "Name: 1, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "phi = model.get_phi()\n",
    "for t in ['0','1']:\n",
    "    top = phi.sort_values([t],ascending=False)[:10]\n",
    "    print(top[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.77386569872958244, 0.77586206896551724)\n",
      "{'min_tf': 1, 'num_back': 1, 'num_topics': 4, 'tau': 1.0}\n"
     ]
    }
   ],
   "source": [
    "best = [0,0]\n",
    "\n",
    "for p in list(pg):\n",
    "    \n",
    "    model = topic_model(p['num_topics'], p['num_back'],p['tau'],p['min_tf'])\n",
    "    model.fit_offline(batch_vectorizer, num_collection_passes=30)\n",
    "    \n",
    "    theta = model.get_theta()\n",
    "    X = theta.as_matrix()[:-p['num_back']].T\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=3).fit(X)\n",
    "    y_pred = kmeans.labels_\n",
    "    y_true = np.array(marks)\n",
    "    \n",
    "    \n",
    "    a,b = without_zero(y_true,X)\n",
    "    res = precision_recall(a,b)\n",
    "    \n",
    "    if res[0] > best[0]:\n",
    "        best = res\n",
    "        best_p = p\n",
    "    \n",
    "print(best)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score_tracker['PerplexityScore'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
