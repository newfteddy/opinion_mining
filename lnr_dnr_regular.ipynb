{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "from nltk import DependencyGraph\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import pymorphy2\n",
    "import math\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import time\n",
    "import codecs\n",
    "import os.path\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_word_list(word_list):\n",
    "    res = []\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    stop_words = get_stop_words('russian')\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        res.append(lemmatizer.parse(word)[0].normal_form.strip())\n",
    "    return res\n",
    "  \n",
    "  \n",
    "def preprocess(filename):\n",
    "    f = codecs.open(filename, 'r')\n",
    "    t = open('lnr_dnr_reg_vw','w')\n",
    "    tokenizer = RegexpTokenizer(r'[а-яА-Я]*')\n",
    "    i = 0\n",
    "    for line in f.readlines():\n",
    "        if line =='\\n':\n",
    "            continue\n",
    "        if i%2 == 0:\n",
    "            words = [x.lower() for x in tokenizer.tokenize(line) if x != '']\n",
    "            words = lemmatize_word_list(words)\n",
    "            t.write(str(i//2) + ' |text ')\n",
    "            for word in set(words):\n",
    "                t.write(str(word) + ':' + str(words.count(word)) + ' ')\n",
    "        else:\n",
    "            t.write('|mark ' + line.split('|mark ')[1])\n",
    "        i+=1\n",
    "    f.close()\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "preprocess('lnr_dnr_labelled.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marks = []\n",
    "f = open('lnr_dnr_reg_vw','r')\n",
    "t = open('news_reg_vw','w')\n",
    "for line in f.readlines():\n",
    "    m = int(line.split('|mark ')[1])\n",
    "    if m == 9:\n",
    "        continue\n",
    "    t.write(line)\n",
    "    marks.append(m)\n",
    "f.close()\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './news_reg_vw'\n",
    "batches_path = './batches/'\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=data_path, collection_name='',\n",
    "                                            data_format='vowpal_wabbit', batch_size = 100, \n",
    "                                            target_folder=batches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path=batches_path, \n",
    "                                         data_format='batches',\n",
    "                                       gather_dictionary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_model(num_of_topics, num_back, tau, tf):\n",
    "    class_ids = {\n",
    "         'text': 1.0\n",
    "    }\n",
    "\n",
    "    names_of_topics = [str(x) for x in range(num_of_topics)]\n",
    "\n",
    "    dictionary_path=batches_path + '/news_dictionary.dict'\n",
    "\n",
    "    my_dictionary = artm.Dictionary()\n",
    "\n",
    "    if os.path.exists(dictionary_path):\n",
    "        os.remove(dictionary_path)\n",
    "        \n",
    "    my_dictionary.gather(data_path=batches_path)\n",
    "    my_dictionary.save(dictionary_path=batches_path + '/news_dictionary')\n",
    "    my_dictionary.load(dictionary_path=batches_path + '/news_dictionary.dict')\n",
    "\n",
    "    my_dictionary.filter(min_tf=tf)\n",
    "\n",
    "    scores_artm = [artm.PerplexityScore(name='PerplexityScore', \n",
    "                                        dictionary=my_dictionary\n",
    "                                       ),\n",
    "                   artm.TopTokensScore(name='TopTokensScore', \n",
    "                                       topic_names=names_of_topics, \n",
    "                                       num_tokens=1000, \n",
    "                                       dictionary=my_dictionary,\n",
    "                                       class_id='text'\n",
    "                                      )]\n",
    "\n",
    "    model = artm.ARTM(num_topics=num_of_topics,\n",
    "                      #reuse_theta=True,\n",
    "                      cache_theta=True,\n",
    "                      num_document_passes=1,\n",
    "                      topic_names=names_of_topics,\n",
    "                      class_ids=class_ids, \n",
    "                      scores=scores_artm,\n",
    "                      #regularizers=regularizers_artm,\n",
    "                      dictionary=my_dictionary)\n",
    "\n",
    "\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhiRegularizer',\n",
    "                                                            class_ids=['@default_class'],\n",
    "                                                            topic_names=model.topic_names[:-num_back],tau = -tau))\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SmoothPhiRegularizer',\n",
    "                                                            class_ids=['@default_class'],\n",
    "                                                            topic_names=model.topic_names[-num_back:],tau = tau))\n",
    "\n",
    "\n",
    "    model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='DecorrelatorRegularizer',\n",
    "                                                          class_ids=['@default_class'],\n",
    "                                                          topic_names=model.topic_names[:-num_back], tau=tau))\n",
    "    model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SparseThetaRegularizer',\n",
    "                                                            topic_names=model.topic_names[-num_back], tau = tau))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_clusters(y_true, y_pred):\n",
    "    m = {}\n",
    "    clusters = set(y_true)\n",
    "    for c1 in clusters:\n",
    "        cnt1 = 0\n",
    "        for c2 in set(y_pred): \n",
    "            \n",
    "            cnt = 0\n",
    "            for (x,y) in zip(y_true,y_pred):\n",
    "                if (x==c1) & (y==c2):\n",
    "                    cnt+=1\n",
    "            if cnt>cnt1:\n",
    "                cnt1 = cnt\n",
    "                res = c2\n",
    "        m[c1] = res\n",
    "    return m\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def precision_recall(y_true,y_pred):\n",
    "    m = map_clusters(y_true,y_pred)\n",
    "    if len(set(m.values()))<len(set(y_true)):\n",
    "        return 0,0\n",
    "    y_true = np.array([m[x] for x in y_true])\n",
    "    precision = metrics.precision_score(y_true,y_pred,average='weighted')\n",
    "    recall = metrics.recall_score(y_true,y_pred,average='weighted')\n",
    "    return precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "(0.60058519793459553, 0.57831325301204817)\n",
      "{'min_tf': 3, 'num_back': 1, 'num_topics': 3, 'tau': 3.0}\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param['num_topics'] = [x for x in range(3,6)]\n",
    "param['num_back'] = [x for x in range(1,3)]\n",
    "param['min_tf'] = [x for x in range(1,4)]\n",
    "param['tau'] = [x for x in np.arange(1.,5.,1.)]\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import cluster\n",
    "\n",
    "pg = ParameterGrid(param)\n",
    "print(len(list(pg)))\n",
    "\n",
    "best = [0,0]\n",
    "\n",
    "for p in list(pg):\n",
    "    \n",
    "    model = topic_model(p['num_topics'], p['num_back'],p['tau'],p['min_tf'])\n",
    "    model.fit_offline(batch_vectorizer, num_collection_passes=30)\n",
    "    \n",
    "    theta = model.get_theta()\n",
    "    X = theta.as_matrix()[:-p['num_back']].T\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=3).fit(X)\n",
    "    y_pred = kmeans.labels_\n",
    "    y_true = np.array(marks)\n",
    "    res = precision_recall(y_true,y_pred)\n",
    "    \n",
    "    if res[0] > best[0]:\n",
    "        best = res\n",
    "        best_p = p\n",
    "    \n",
    "print(best)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def without_zero(y_true,X):\n",
    "    a, b = [],[]\n",
    "    for (x,y) in zip(y_true,X):\n",
    "        if x ==0:\n",
    "            continue\n",
    "        a.append(x)\n",
    "        b.append(y)\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=2).fit(b)\n",
    "    return np.array(a),np.array(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75862068965517238, 0.75862068965517238)\n",
      "{'min_tf': 3, 'num_back': 1, 'num_topics': 3, 'tau': 3.0}\n"
     ]
    }
   ],
   "source": [
    "best = [0,0]\n",
    "\n",
    "for p in list(pg):\n",
    "    \n",
    "    model = topic_model(p['num_topics'], p['num_back'],p['tau'],p['min_tf'])\n",
    "    model.fit_offline(batch_vectorizer, num_collection_passes=30)\n",
    "    \n",
    "    theta = model.get_theta()\n",
    "    X = theta.as_matrix()[:-p['num_back']].T\n",
    "\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=3).fit(X)\n",
    "    y_pred = kmeans.labels_\n",
    "    y_true = np.array(marks)\n",
    "    \n",
    "    \n",
    "    a,b = without_zero(y_true,X)\n",
    "    res = precision_recall(a,b)\n",
    "    \n",
    "    if res[0] > best[0]:\n",
    "        best = res\n",
    "        best_p = p\n",
    "    \n",
    "print(best)\n",
    "print(best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
